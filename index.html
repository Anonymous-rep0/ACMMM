<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
  <style>
        .method {
            display: inline-block;
/*             width: 120px; /* Adjust the width as needed */ */
            font-weight: bold;
        }

        .explanation {
            display: inline-block;
/*             margin-left: 20px; /* Adjust the margin as needed */ */
        }
    </style>
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->


  </section>

  <section class="main-content">
    <h1 id="">
      <center>FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech</center>
    </h1>

    <h3 id="">
      <center>A work submitted to ACMMM 2025</center>
    </h3>



    <br><br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p>Current speech generation research can be categorized into two primary classes: non-autoregressive (NAR) and 
      autoregressive (AR). The fundamental distinction between these approaches lies in the duration prediction strategy
      employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and 
      independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to 
      predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves 
      prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of 
      stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. 
      The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration 
      predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. 
      Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic 
      model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference 
      audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic 
      variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre.
      Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, 
      when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with 
      about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.</p>
    <br><br>

        <center><img src='raw/fig/overall.png' width="70%"></center>
        <br><br>
        <center><span><b>Figure 1. Overview of FlexSpeech. (a). The architecture of FlexSpeech acoustic model. 
          The phoneme embeddings, repeated speaker embeddings, and random noise are concatenated along the channel 
          dimension to form the input to the model, which then predicts a mel-spectrogram of the same length. 
          (b). Duration model and preference alignment. The decoder predicts current phoneme duration in an 
          autoregressive manner, based on the hidden state and previous phoneme durations.</b></span></center>

    <br><br>


    <h2>2. Demos <a name="Comparison"></a></h2>
    <!-- <p>Methods</p>
    <ul>
      <li><b><a href="https://github.com/fishaudio/Bert-VITS2">VITS : </a></b>a VITS2 with multilingual BERT<sup>1</sup></li> 
      <li><b>TACA-VITS : </b>A Text-Aware and Context-Aware VITS</li>
      <li><b>LM : </b>A LM with Hubert token</li>        
      <li><b>TACA-VITS : </b> A Text-Aware and Context-Aware LM-based TTS</li>


    </ul> -->

    <table>
      <tbody id="tbody_speech">
      </tbody>
    </table>



</html>

<script type="" text/javascript>
  function short_form() {
    let scenes = [
      [0],[1],[2],[3],[4],[5]
          ]
    let models = ["UNet-CV(Non-Streaming)", "DiT-CV(Non-Streaming)",  "DiT-CVS", "StreamFlow-SR","StreamFlow-LR"]
    short_data = ""
    for (const id in models) {
      model = models[id]
  
      short_data += '<td style="text-align: center; width: 14%;" rowspan=1><strong>' + model  + '<strong></td>'
    }
    short_data += `</tr>`
    
    for (let x in scenes) {
      let scene = scenes[x]
      let file = scene[0]
      let scene_data = ""

      scene_data += '<tr>'
      for (let z in models) {
        let model = models[z]
        scene_data += '<td style="text-align: center"><audio style="width: 100%;" controls="" src="' + './raw/samples/' + model + '/' + file + '.wav' + '"></audio></td>'
        
      }
      scene_data += '</tr>'
      short_data += scene_data
    }
    return short_data
  }
    window.onload = function () {
    document.getElementById('tbody_speech').innerHTML = short_form()
  }
 
</script> 

